---
title: "Lecture 1.3 - Advanced distributions"
subtitle: "DKU Stats 101 Spring 2024"
author: "Professor MacDonald"
date: "3/25/2024"
output: 
  learnr::tutorial:
    toc_depth: 2
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

library(tidyverse)
library(gridExtra)
library(learnr)
library(knitr)

theme_set(theme_classic())

titanic_survival <- read.csv("www/titanic_survival.csv")
classroster <- read.csv("www/classroster.csv", fileEncoding="UTF-8-BOM")
nbaboxscores <- read.csv("www/nba_games_short.csv")

titanic_survival <- titanic_survival %>% 
  rename(name = v1)

titanic_mean <- mean(titanic_survival$age)
titanic_sd <- sd(titanic_survival$age)
```

# More on distributions

## Thoughts about comparing groups

-   Faceted histograms are a reasonable display to show distributions by a categorical variable
    -   However these displays become hard to interpret when the number of levels in a category grows large
-   Much easier to interpret is side by side box plots
-   Box plots capture many important characteristics of a distribution into a summary display
-   Think carefully about how you treat outliers
-   Let's view data from the 2022-2023 NBA season

## Two group comparison

### NBA side-by-side histograms of points scored by W/L

```{r nbacomparison-hist, exercise=TRUE}
ggplot(nbaboxscores, aes(x=PTS_home)) +
  geom_histogram(fill="blue4") +
  labs(x="Points scored by home team", y="Count") +
  facet_wrap(~HOME_TEAM_WINS)
```

### NBA boxplot comparison of points scored by W/L

```{r nbacomparison-box, exercise=TRUE}
ggplot(nbaboxscores, aes(x=HOME_TEAM_WINS,y=PTS_home)) + 
  geom_boxplot() + 
  labs(x="Home team result", y="Points scored")
```

### NBA boxplot comparison of points scored by W/L (better)

```{r nbacomparison-betterbox, exercise=TRUE}
ggplot(nbaboxscores, aes(x=HOME_TEAM_WINS,y=PTS_home)) + 
  geom_boxplot(fill="light blue") + 
  labs(x="Home team result", y="Points scored") + 
  coord_flip()
```

## Many group comparison

### NBA side-by-side histograms of points scored by team

```{r nbateamcomparison-hist, exercise=TRUE}
ggplot(nbaboxscores, aes(x=PTS_home)) +
  geom_histogram(fill="blue4") +
  labs(x="Points scored by home team", y="Count") +
  facet_wrap( ~ as.factor(HOME_TEAM_ID))
```

### NBA boxplot comparison of points scored by team (better)

```{r nbateamcomparison-box, exercise=TRUE}
ggplot(nbaboxscores, aes(x=as.factor(HOME_TEAM_ID),y=PTS_home, fill=DIV_HOME)) + 
  geom_boxplot() + 
  labs(x="Team", y="Points scored by home team") +
   scale_x_discrete(guide = guide_axis(n.dodge=2))
```

### Your turn

- Work with your neighbor to analyze a different set of statistics
  + Can be by division or not
  + Remember the key features of distributions
    - Shape
    - Center
    - Spread
- Interpret your results

```{r picker0-1, exercise=TRUE}
sample(classroster$name, 1)
```

## Checking outliers - assists

### Outliers - assists

```{r blocksoutliers, exercise=TRUE}
ggplot(nbaboxscores, aes(x=AST_home)) +
  geom_histogram(fill="blue4", binwidth=1) +
  labs(x="Assists by home team", y="Count")
```

### Assists \> 40 - true outliers?

```{r blocksoutlierscheck, exercise=TRUE}
nbaboxscores %>% 
    filter(AST_home > 40) %>%
    select(GAME_DATE_EST, HOME_TEAM_ID, VISITOR_TEAM_ID, PTS_home, REB_home)
```

```{r picker0-2, exercise=TRUE}
sample(classroster$name, 1)
```

## Checking outliers - away points

### Outliers - away points

```{r pointsoutliers, exercise=TRUE}
ggplot(nbaboxscores, aes(x=PTS_away)) +
  geom_histogram(fill="blue4", binwidth=1) +
  labs(x="Points", y="Count")
```

### Points by away team \> 140 - true outliers?

```{r pointsoutlierscheck, exercise=TRUE}
nbaboxscores %>% 
    filter(PTS_away > 140) %>%
    select(GAME_DATE_EST, HOME_TEAM_ID, VISITOR_TEAM_ID, PTS_away, REB_away)
```

```{r picker0-3, exercise=TRUE}
sample(classroster$name, 1)
```

## In summary

-   Think about which kind of display is appropriate for comparing distributions
-   When conditioning on a categorical variable, boxplots are usually better
-   But boxplots lose information
-   Think carefully about omitting outliers
-   Outliers may reveal important information about your dataset!

## Titanic passengers and the Normal distribution

![Titanic](www/Titanic.jpg){width="800"}

### Dataset of passengers on the Titanic

```{r datastructure, exercise=TRUE}
kable(head(titanic_survival))
```

-   What are your expectations for how age should be distributed?

```{r picker1, exercise=TRUE}
sample(classroster$name, 1)
```

-   We are going to violate our first three rules:
    1.  Make a picture
    2.  Make a picture
    3.  Make a picture

### Were the passenger ages normally distributed?

To answer that question, we need some information about the distribution

Remember, our main information about distributions is:

-   Shape

-   Center

-   Spread

### Information about `age`

-   Standard deviation: `r round(sd(titanic_survival$age), digits=1)`
-   Mean: `r round(mean(titanic_survival$age), digits=1)`
-   Normal model: $N(\mu, \sigma) = N(`r round(mean(titanic_survival$age), digits=1)`,`r round(sd(titanic_survival$age), digits=1)`)$
    -   $\mu$ is the theoretical mean
    -   $\sigma$ is the theoretical standard deviation
    -   These values define the data generating process
    -   We only see some values of the data generating process, but if we saw infinite values, the mean would be $\mu$ and the sd would be $\sigma$
    -   More on this in the second half of class
-   How can we check normality using this information?

```{r picker3, exercise=TRUE}
sample(classroster$name, 1)
```

## Checking normality

### Thinking about normality

-   We can check normality by comparing the quantiles of our data with that of the known quantiles of the normal distribution
    -   We know approximately 95% of the data lies within two standard deviations
    -   Therefore, 2.5% data with the lowest values lie outside of -2 standard deviations and 2.5% of data with the highest values lie outside of 2 standard deviations
-   Similarly, we know the same information for data within one standard deviation (16%, 68%, 16%)

### Data within standard deviations

```{r agesd, exercise=TRUE}
norm.dist <- rnorm(1000000)

ds.low <- density(norm.dist, from=min(norm.dist), to=-2)
ds.high <- density(norm.dist, from=2, to=max(norm.dist))
ds.low.mid <- density(norm.dist, from=min(norm.dist), to=-1)
ds.high.mid <- density(norm.dist, from=1, to=max(norm.dist))

ds.low.data <- data.frame(x = ds.low$x, y = ds.low$y)
ds.low.mid.data <- data.frame(x = ds.low.mid$x, y = ds.low.mid$y)
ds.high.data <- data.frame(x = ds.high$x, y = ds.high$y)
ds.high.mid.data <- data.frame(x = ds.high.mid$x, y = ds.high.mid$y)

two.sd <- ggplot(data.frame(norm.dist), aes(norm.dist)) + 
  geom_density() + 
  geom_vline(xintercept=-2, color="dark red") +
  geom_vline(xintercept=2, color="dark red") +
  geom_area(data = ds.low.data, aes(x = x, y = y), fill="blue4", color="blue4") +
  geom_area(data = ds.high.data, aes(x = x, y = y), fill="blue4", color="blue4") + 
  labs(x="Standard deviations", y="Density") + 
  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), limits=(c(-4, 4))) +
  annotate(geom="text", x=-3, y=0.1, label="~2.5% of the data") +
  annotate(geom="text", x=3, y=0.1, label="~2.5% of the data") +
  annotate(geom="text", x=0, y=0.1, label="~95% of the data") 
  
one.sd <- ggplot(data.frame(norm.dist), aes(norm.dist)) +
  geom_density() +
  geom_vline(xintercept = -1, color="dark red") +
  geom_vline(xintercept = 1, color="dark red") +
  geom_area(data = ds.low.mid.data, aes(x = x, y = y), fill="blue4", color="blue4") +
  geom_area(data = ds.high.mid.data, aes(x = x, y = y), fill="blue4", color="blue4") +
  labs(x="Standard deviations", y="Density") + 
  scale_x_continuous(breaks=c(-3, -2, -1, 0, 1, 2, 3), limits=(c(-4, 4))) +
  annotate(geom="text", x=-2.5, y=0.2, label="~16% of the data") +
  annotate(geom="text", x=2.5, y=0.2, label="~16% of the data") +
  annotate(geom="text", x=0, y=0.2, label="~68% of the data") 

grid.arrange(two.sd, one.sd)
```

## Checking against the data

### Histogram of ages from the data

```{r agehist, exercise=TRUE}
norm.dist.titanic <- rnorm(1000000, mean=titanic_mean, sd=titanic_sd)
norm.dist.titanic <- data.frame(norm.dist.titanic)

ggplot(titanic_survival, aes(x=age)) + 
  geom_histogram(aes(y=..density..), fill="blue4") +
  geom_vline(xintercept=titanic_mean, color="dark red") + 
  geom_density(data=norm.dist.titanic, aes(norm.dist.titanic), color="dark red") +
  labs(x="Age", y="Count") +
  scale_x_continuous(limits=c(-10, 80))
```

### Normality and scaling

-   Note that normality does not depend on the size of the standard deviation or the size of the mean
-   Could easily change the units to be months instead of years
    -   Mean would increase a lot
    -   Standard deviation would increase a lot
    -   However, amount of observations within each standard deviation would stay the same

## Final thoughts on normality

### When is the normal distribution useful?

-   When we know a data-generating process is normally distributed we don't even need to sample the population
    -   Can find out exactly how much data is between a certain number of standard deviations
-   When we expect a data-generating process to be normally distributed, can test for deviations from normality
    -   In the case of Titanic passengers, some parts of the distribution were more bunched up, others more spread out
-   A lot of our statistical techniques require or work better when the data is 'roughly' normal
    -   Will detail these in the coming weeks
-   We can transform our data to be closer to normal
    -   Note that transformations won't work if the data has multiple modes, can only correct skew

### What transformation would be helpful for `age`?

```{r agehist2, exercise=TRUE}
ggplot(titanic_survival, aes(x=age)) + 
  geom_histogram(fill="blue4") +
  labs(x="Age", y="Count")
```

```{r picker5, exercise=TRUE}
sample(classroster$name, 1)
```
